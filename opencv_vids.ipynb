{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cd9fa4ce"
   },
   "source": [
    "Code to dowload traffic videos via OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "42a5e3c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/pmjohnson/Documents/yolov4-deepsort_pmj\r\n"
     ]
    }
   ],
   "source": [
    "#Import\n",
    "import cv2\n",
    "import numpy as np\n",
    "import time\n",
    "from datetime import datetime\n",
    "import sys,os #relates to file names, paths, and directories\n",
    "!pwd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7c0baf73"
   },
   "source": [
    "The code below opens a video stream, downsamples, and writes it to an output file --> use this to capture part of a video stream to then input as a file input to a model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "12cb58fc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5276.0 frames in 276.91990542411804 seconds -- approx. FPS: 19.052440422870706\n",
      "Capture and write objects closed.\n"
     ]
    }
   ],
   "source": [
    "### Open video stream and write output (without object detection):\n",
    "### Uncomment whichever CAP object you want to use and specify PRIMARY channel \n",
    "\n",
    "## Video capture objects\n",
    "# 21st and Broadway (Primary-0 parking lot, Primary-1 pole obstruction, *Primary-2 intersection)\n",
    "cap = cv2.VideoCapture('rtsp://aot:K%K4@lNEqW@10.32.136.8/defaultPrimary-2?streamType=u',cv2.CAP_FFMPEG)\n",
    "\n",
    "# 21st and West End (*Primary-0 intersection WestEnd N., *Primary-1 intersection 21st E., Primary-2 campus walk)\n",
    "#cap = cv2.VideoCapture('rtsp://aot:K%K4@lNEqW@10.32.144.14/defaultPrimary-0?streamType=u',cv2.CAP_FFMPEG)\n",
    "\n",
    "# MRB III (Primary-0 crosswalk, Primary-1 sidewalk, *Primary-2 intersection)\n",
    "#cap = cv2.VideoCapture('rtsp://aot:K%K4@lNEqW@10.18.26.28/defaultPrimary-2?streamType=u',cv2.CAP_FFMPEG)\n",
    "\n",
    "\n",
    "\n",
    "# Check if camera opened successfully (note - need to be on Vanderbilt VPN)\n",
    "if (cap.isOpened() == False): \n",
    "  print(\"Unable to read camera feed\")\n",
    "\n",
    "# Default resolutions of the frame are obtained.The default resolutions are system dependent.\n",
    "# We convert the resolutions from float to integer.\n",
    "#NOTE: \"You can also access some of the features of this video using cap.get(propId) method where \n",
    "#propId is a number from 0 to 18. \n",
    "#Each number denotes a property of the video (if it is applicable to that video)\"\n",
    "#PropID list: https://docs.opencv.org/2.4/modules/highgui/doc/reading_and_writing_images_and_video.html#videocapture-get \n",
    "\n",
    "frame_width = int(cap.get(3)) #3 refers to width (current: 1920)\n",
    "frame_height = int(cap.get(4)) #4 is height (current: 1080)\n",
    "frame_rate = int(cap.get(5)) #5 is frame rate (current: 15 f/sec)\n",
    "\n",
    "# Define the codec and create VideoWriter object.\n",
    "out = cv2.VideoWriter('/home/pmjohnson/Downloads/test_outputLONG540p.mp4',cv2.VideoWriter_fourcc(*'H264'),frame_rate,(frame_width,frame_height))\n",
    "start = time.time()\n",
    "ret, frame = cap.read()\n",
    "\n",
    "\n",
    "frame_idx = 0\n",
    "skip = 4  # process every 5 frames (Derek set to 5 initially, 1 for super slow)\n",
    "while ret:\n",
    "  \n",
    "    \n",
    "    # detect objects with YoloV4\n",
    "    if frame_idx % skip == 0:\n",
    "    \n",
    "        # downsample resolution because the GPU doesn't have enough memory otherwise (set to 4 initially)\n",
    "        downsample = 1.5 #1 --> 1080p, 1.5--> 720p, 2.25 --> 480p, 4 --> 270p\n",
    "        im_det = cv2.resize(frame,(int(frame.shape[1]//downsample),int(frame.shape[0]//downsample)))\n",
    "#         detections = detect(im_det,\"PyTorch_YOLOv4/cfg/yolov4.cfg\",\"PyTorch_YOLOv4/weights/yolov4.weights\")[0].data.cpu().numpy()\n",
    "#         torch.cuda.empty_cache()\n",
    "#         detections[:,:4] *= downsample # rescale \n",
    "        \n",
    "#         # plot each of the detections\n",
    "#         for bbox in detections:\n",
    "#             label = \"{}: {:.2f}\".format(class_names[int(bbox[5])],bbox[4])\n",
    "#             color = colors[int(bbox[5])]\n",
    "#             color = (int(color[0]),int(color[1]),int(color[2]))\n",
    "            \n",
    "#             c1 =  (int(bbox[0]),int(bbox[1]))\n",
    "#             c2 =  (int(bbox[2]),int(bbox[3]))\n",
    "#             cv2.rectangle(frame,c1,c2,color,thickness = 1)\n",
    "            \n",
    "#             # plot label\n",
    "#             text_size = 0.8\n",
    "#             t_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_PLAIN,text_size , 1)[0]\n",
    "#             c2 = c1[0] + t_size[0] + 3, c1[1] + t_size[1] + 4\n",
    "#             cv2.rectangle(frame, c1, c2,color, -1)\n",
    "#             cv2.putText(frame, label, (c1[0], c1[1] + t_size[1] + 4), cv2.FONT_HERSHEY_PLAIN,text_size, [225,255,255], 1)\n",
    "    \n",
    "    \n",
    "        # Write the frame into output file\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the resulting frame    \n",
    "        cv2.imshow('frame',frame)\n",
    "        \n",
    "        # Press Q on keyboard to stop recording\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'): #wait 1 milisecond unless q is pressed...\n",
    "          break\n",
    "\n",
    "\n",
    "    # get next frame\n",
    "    ret, frame = cap.read()\n",
    "    frame_idx += 1\n",
    "\n",
    "# release video write and read objects \n",
    "n_frames = cap.get(1) -1\n",
    "cap.release()\n",
    "out.release()\n",
    "\n",
    "end = time.time()\n",
    "elapsed = end - start\n",
    "\n",
    "print('{} frames in {} seconds -- approx. FPS: {}'.format(n_frames,elapsed,n_frames/elapsed))\n",
    "#print('Stream dimensions:{} x {}'.format(frame_width,frame_height))\n",
    "\n",
    "# close all frames\n",
    "cv2.waitKey(1)\n",
    "cv2.destroyAllWindows()\n",
    "cv2.waitKey(1)\n",
    "print(\"Capture and write objects closed.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
